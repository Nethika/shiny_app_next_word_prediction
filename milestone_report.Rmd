---
title: "Coursera - Data Science Capstone - Milestone Report"
author: "Nethika Suraweera"
date: "August 4, 2018"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction

This report consist of an exploratory analysis for the data that is being used to generate a word prediction model. The goal is to build a  basic n-gram model for predicting the next word based on the previous 1, 2, or 3 words. The data are from 3 different sources: 1. Blogs, 2. Twitter 3. News. In this intermidiatory stage of the project, the data will be sampled, preprocessed and tokenized to generate n-grams. The report also incudes the future planning for generating a word prediction App.

*** 

## Loading Data and Exploratory analysis 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(ngram)
library(stringr)
library(qdapRegex)
library(tm)
library(reshape)
library(ggplot2)
library(gridExtra)
library(gtable)



file_blogs = "./final/en_US/en_US.blogs.txt"
file_twitter = "./final/en_US/en_US.twitter.txt"
file_news = "./final/en_US/en_US.news.txt"

list_paths = list(blogs = file_blogs, twitter = file_twitter, news = file_news)

# function to size of files
fn_size <- function(file_name){
  f_size<-file.info(file_name)
  size_kb = f_size$size/1024
  size_mb = size_kb/1024
  return(size_mb)
}

# function to word count
fn_words <- function(text){
  wordcount(text, sep = " ", count.function = sum)
}

# function to read lines
fn_rlines <- function(file_name){
  con <- file(file_name)
  f_lines <- readLines(con, encoding = "UTF-8", skipNul = TRUE)
  close(con)
  return(f_lines)
}

# function to write lines
fn_wlines <- function(list_lines,file_name){
  fileConn<-file(file_name)
  writeLines(list_lines, fileConn)
  close(fileConn)
}

# function to sample data
fn_sample <- function (x,perc){
  #rbinom(x, 1, prob = percent)
  sample(x, length(x) * perc)
} 

# files paths 
blogs <- fn_rlines(file_blogs)
twitter <- fn_rlines(file_twitter)
news <- fn_rlines(file_news)


# put corpura into a list
list_corpora <- list(blogs = blogs, twitter = twitter, news = news)


# data frame to store counts
df_corpora <- data.frame(source = c("blogs", "twitter", "news"), size_MB = NA, line_count = NA, word_count = NA)

# get line count and word count for each Corpura
df_corpora$size_MB <- sapply(list_paths, fn_size)
df_corpora$line_count <- sapply(list_corpora, length)
df_corpora$word_count <- sapply(list_corpora, fn_words)

rm(list_corpora)
```

### Summary of Raw Data:
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
library(knitr)
kable(df_corpora)
```

### Plots to Represent Raw Data:
```{r, echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# Plot data
# plot size
p_size<- ggplot(df_corpora, aes(x = factor(source), y = size_MB, fill = source))
p_size<- p_size + geom_bar(stat = "identity") +
  labs(y = "MB", x = "", title = "File Size (MB)") 

# plot lines
p_lines <- ggplot(df_corpora, aes(x = factor(source), y = line_count/1e+06, fill = source))
p_lines <- p_lines + geom_bar(stat = "identity") +
  labs(y = "Lines (millions)", x = "", title = "Lines Count")

# plot words
p_words<- ggplot(df_corpora, aes(x = factor(source), y = word_count/1e+06,fill = source))
p_words <- p_words + geom_bar(stat = "identity") +
  labs(y = "Words (millions)", x = "", title = "Words Count") 


legend = gtable_filter(ggplot_gtable(ggplot_build(p_size)), "guide-box")

grid.arrange(p_size + theme(legend.position="none"), 
             p_lines+ theme(legend.position="none"), 
             p_words+ theme(legend.position="none"), 
             widths = c(1.5,1.5,1.5,0.7),
             legend, 
             nrow = 1)
```

*** 

## Sampling Data

5% of data is being randomly sampled to represent the larger data set. 
Following table shows the summary of the samples.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# create a list of random variables
set.seed(123)
percent <- 0.05

blogs_sample <- fn_sample(blogs,percent)
twitter_sample <- fn_sample(twitter,percent)
news_sample <- fn_sample(news,percent)

rm(blogs)
rm(twitter)
rm(news)

# create a list to store samples
list_sample <- list(blogs = blogs_sample, twitter = twitter_sample , news = news_sample)

# create a data frame for samples
df_sample <- data.frame(source = c("blogs", "twitter", "news"), line_count = NA, word_count = NA)

# get counts of samples
df_sample$line_count <- sapply(list_sample, length)
df_sample$word_count <- sapply(list_sample, fn_words)

rm(list_sample)
kable(df_sample)

```

*** 

## Preprocessing Data

Data preprocessing does the following tasks:

* Removing URLs
* Removing Hash tags
* Removing Twitter handlers
* Removing non Ascii characters
* Converst to lower case
* Removing Profanity words (Used 451 words/phrases from the 'Full List of Bad Words and Top Swear Words Banned by Google')
* Removing punctuation
* Removing numbers
* Removing stop words
* Removing/fixing white spaces


```{r, echo=TRUE, message=FALSE, warning=FALSE}
# Preprocess Data

### helper functions
stringi_toLower <- function(x) stringi::stri_trans_tolower(x)
remove_URL <- function(x) gsub("http:[[:alnum:]]*", "", x)
remove_HashTags <- function(x) gsub("#\\S+", "", x)
remove_TwitterHandles <- function(x) gsub("@\\S+", "", x)
remove_nonAscii <- function(x) gsub("[^\x01-\x7F]", "", x)
fix_whitespaces <- function(x) qdapRegex::rm_white(x)
## List of Bad Words and Top Swear Words Banned by Google 
profanity_words <- fn_rlines("list.txt")
  
# function to Preprocess
fn_preprocess <- function(list_text){
  corpus_text <- tm::Corpus(VectorSource(list_text))
  corpus_text <- tm::tm_map(corpus_text, content_transformer(remove_URL))
  corpus_text <- tm::tm_map(corpus_text, content_transformer(remove_HashTags))
  corpus_text <- tm::tm_map(corpus_text, content_transformer(remove_TwitterHandles))
  corpus_text <- tm::tm_map(corpus_text, content_transformer(remove_nonAscii))
  corpus_text <- tm::tm_map(corpus_text, content_transformer(stringi_toLower))
  corpus_text <- tm::tm_map(corpus_text, removeWords, stopwords("en"))
  corpus_text <- tm::tm_map(corpus_text, removeWords, profanity_words)
  corpus_text <- tm::tm_map(corpus_text, removePunctuation)
  corpus_text <- tm::tm_map(corpus_text, removeNumbers)
  corpus_text <- tm::tm_map(corpus_text, content_transformer(fix_whitespaces))
  return (corpus_text)

}

Encoding(blogs_sample) <- "UTF-8"
Encoding(twitter_sample) <- "UTF-8"
Encoding(news_sample) <- "UTF-8"

corpus_blogs <- fn_preprocess(blogs_sample)
corpus_twitter <- fn_preprocess(twitter_sample)
corpus_news <- fn_preprocess(news_sample)

rm(blogs_sample)
rm(twitter_sample)
rm(news_sample)
```

*** 

## N-Gram Tokenization

R's package 'ngram' is used to tokenize the data.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# ngram tokenizing

str_blogs <- concatenate ( lapply ( corpus_blogs , "[", 1) )
str_twitter <- concatenate ( lapply ( corpus_twitter , "[", 1) )
str_news <- concatenate ( lapply ( corpus_news , "[", 1) )

rm(corpus_blogs)
rm(corpus_twitter)
rm(corpus_news)

full_string <- concatenate (str_blogs,str_twitter,str_news)

rm(str_blogs)
rm(str_twitter)
rm(str_news)

## 1-grams
ng1 <- ngram (full_string , n =1)
df_1gram <- get.phrasetable ( ng1 )
df_1gram <- df_1gram[order(df_1gram$freq,decreasing = TRUE),]

## 2-grams
ng2 <- ngram (full_string , n =2)
df_2gram <- get.phrasetable ( ng2 )

## 3-grams
ng3 <- ngram (full_string , n =3)
df_3gram <- get.phrasetable ( ng3 )

## 4-grams
ng4 <- ngram (full_string , n =4)
df_4gram <- get.phrasetable ( ng4 )

```

The word frequencies for 1-grams, 2-grams and 3-grams are shown in the following plots (first 30 highest frequency phrases are plotted):


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Plot ngrams
theme_set(theme_bw())
p_1gram<- ggplot(df_1gram[1:30,], aes(x=reorder(ngrams, -freq), y=freq)) + 
  geom_bar(stat="identity", width=.5, fill="tomato3") + 
  labs(title="1-Grams", 
       y="n-gram frequency",
       x="n-gram") + 
  theme(axis.text.x = element_text(angle=65, vjust = 1, hjust = 1))

p_2gram<- ggplot(df_2gram[1:30,], aes(x=reorder(ngrams, -freq), y=freq)) + 
  geom_bar(stat="identity", width=.5, fill="tomato3") + 
  labs(title="2-Grams", 
       y="n-gram frequency",
       x="n-gram") + 
  theme(axis.text.x = element_text(angle=65, vjust = 1, hjust = 1))

p_3gram<- ggplot(df_3gram[1:30,], aes(x=reorder(ngrams, -freq), y=freq)) + 
  geom_bar(stat="identity", width=.5, fill="tomato3") + 
  labs(title="3-Grams", 
       y="n-gram frequency",
       x="n-gram") + 
  theme(axis.text.x = element_text(angle=75, vjust = 1, hjust = 1))

p_4gram<- ggplot(df_4gram[1:30,], aes(x=reorder(ngrams, -freq), y=freq)) + 
  geom_bar(stat="identity", width=.5, fill="tomato3") + 
  labs(title="4-Grams", 
       y="n-gram frequency",
       x="n-gram") + 
  theme(axis.text.x = element_text(angle=90, vjust = 1, hjust = 1))

p_1gram
p_2gram
p_3gram
p_4gram

```

*** 

## Future Planning

Some importants points to consider:

* An interactive app that predicts the next word when user entering words.
* builing a predictive model using the tokenized data.
* handling the words come from foreign languages.
* handling the words that may not be in the corpora (backoff models).
* efficiency of the model (Runtime/Size).



